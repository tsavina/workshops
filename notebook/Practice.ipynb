{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\r\n",
    "\r\n",
    "## 1. [Introduction](#s1)\r\n",
    "\r\n",
    "## 2. [OpenVINO™ Overview](#s6) - Kashchikhin\r\n",
    "\r\n",
    "## 3. [OpenVINO™ Deep Learning Workbench](#s7) - Kashchikhin\r\n",
    "\r\n",
    "## 4. [OpenVINO(TM) API](#s7) – Tugaryov\r\n",
    "\r\n",
    "Object Detection sample: http://127.0.0.1:5665/jupyter/lab/tree/tutorials/object_detection_ssd/tutorial_object_detection_ssd.ipynb\r\n",
    "W/o downloader and w/updated cells\r\n",
    "\r\n",
    "## 5. [Practice](#s15) – Tugaryov\r\n",
    "\r\n",
    "Task 1: apply pre-defined blur method to given image at inferred coordinates (photo with several faces)\r\n",
    "\r\n",
    "Task 2: add blurring logic to pre-defined video processor\r\n",
    "\r\n",
    "Task 3: replace each face on the photo with a smile with corresponding emotion\r\n",
    "\r\n",
    "Task 4: emotional smile on the video\r\n",
    "\r\n",
    "** Task 5: Telegram Bot - Smiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro\r\n",
    "### Workshop Contributors\n",
    "\n",
    "   <figure style=\"display: inline-block;\">\n",
    "      <img src=\"pictures/Demidovskij.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption>Alexander Demidovskij</figcaption>\n",
    "    </figure>\n",
    "    \n",
    "   <figure style=\"display: inline-block;\">\n",
    "      <img src=\"pictures/Tarkan.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption>Yaroslav Tarkan</figcaption>\n",
    "    </figure>\n",
    "    \n",
    "   <figure style=\"display: inline-block;\">\n",
    "      <img src=\"pictures/Tugaryov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption>Artyom Tugaryov</figcaption>\n",
    "    </figure>\n",
    "    \n",
    "   <figure style=\"display: inline-block;\">\n",
    "      <img src=\"pictures/Kashchikhin.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption>Andrej Kashchikhin</figcaption>\n",
    "    </figure>\n",
    "    \n",
    "   <figure style=\"display: inline-block;\">\n",
    "      <img src=\"pictures/Savina.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption>Tatiana Savina</figcaption>\n",
    "    </figure>\n",
    "\n",
    "\r\n",
    "### What You Will Learn\r\n",
    "\r\n",
    "Welcome to the workshop! ... We will use OpenVINO™ framework and its graphical interface Deep Learning Workbench to make your Deep Learning journey easy and exciting. \r\n",
    "\r\n",
    "During this workshop you will:\r\n",
    "\r\n",
    "1. Learn the basics of neural model analysis and optimization:\r\n",
    "    - what a model is and how it works\r\n",
    "    - how to measure its performance and analyze the quality\r\n",
    "    - how to tune the model for enhanced performance\r\n",
    "2. Write your own AI application __________\r\n",
    "\r\n",
    "And good luck from our team: \r\n",
    "\r\n",
    "### Why Deep Learning\r\n",
    "\r\n",
    "Deep Learning is highly popular right now due to significant breakthroughs in the artificial neural networks area, which have encouraged businesses to use deep learning solutions as part of their strategy. From digital assistants and chatbots in customer service to object recognition in retail, and much more, Deep learning has enabled the development of various revolutionary AI applications. With Deep Learning, the algorithm does not need to be taught about the essential features. It can discover features from data on its own using a neural network. The exceptional performance of Deep Learning algorithms with difficult tasks involving massive quantities of data, coupled with the growing availability of pre-trained models, has made Deep Learning very appealing to many companies.\r\n",
    "Example\r\n",
    "\r\n",
    "### What Is Inference \r\n",
    "\r\n",
    "While training is the process of teaching a model to perform a particular AI task, Deep learning inference is the process of using a trained model to make predictions against previously unseen data. A trained model is often modified and simplified before being deployed. Some neural models can be large and complex, with hundreds of layers of artificial neurons and weights connecting them. The larger the model, the more memory and energy is consumed to run it, and the longer will be the response time (or latency) from when you input data to the model until you receive a result.\r\n",
    "\r\n",
    "![](pictures/deep-learning.png)\r\n",
    "\r\n",
    "Author: Mark Robins, Intel, [Link](https://www.intel.ru/content/www/ru/ru/artificial-intelligence/posts/deep-learning-training-and-inference.html)\r\n",
    "\r\n",
    "\r\n",
    "But sometimes the use case requires that inference run very fast or at very low power. For example, a self-driving car must be able to detect and respond within milliseconds in order to avoid an accident.In such cases, there is a desire to simplify the model after training to reduce power and latency, even if this simplification results in a slight reduction in accuracy. There are several ways to optimize a model. Further, we will use optimization method called INT8 calibration, which involves reducing the numerical precision of the weights from, for example, 32-bit floating point numbers down to 8-bit, resulting in a reduced model size and faster computation.\r\n",
    "\r\n",
    "Let's take a look at the inference on the real-life example of vehical detection model. The model running alongside the traffic camera constantly processes a video to detect vehicles entering the intersection. If a vehicle enters when the light is red, multiple images of the vehicle are captured and fed into the model, which finds an image that includes a license plate and transmits it for further processing. At the server, a first inference is run to localize the license plate in the image, and a second inference is run to read the characters on the license plate. Finally, the license plate information is sent to the data center where an application looks up the car’s owner based on the license plate and detects up potential traffic violations to be reviewed.\r\n",
    "\r\n",
    "![](pictures/system.png)\r\n",
    "\r\n",
    "Author: Mark Robins, Intel, [Link](https://www.intel.ru/content/www/ru/ru/artificial-intelligence/posts/deep-learning-training-and-inference.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OpenVINO™ Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenVINO™ toolkit is a comprehensive toolkit for optimizing pretrained deep learning models of various use cases to achieve high performance and prepare them for deployment on Intel® platforms. Based on latest generations of artificial neural networks, including Convolutional Neural Networks (CNNs), recurrent and attention-based networks, the toolkit extends computer vision and non-vision workloads across Intel® hardware, maximizing performance. It accelerates applications with high-performance, AI and deep learning inference deployed from edge to cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/about_vino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO™ Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_toolkit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO™ Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO™ Deep Learning Models\r\n",
    "\r\n",
    "A model is a network that has been trained over a set of data using a certain framework. Since Deep learning technologies are used in various industrial\r\n",
    "applications, it is crucial to have an effective solution for each specific use case. OpenVINO Open Model Zoo provides a range of public and Intel pre-trained models to resolve a variety of different tasks, such as classification, object detection, segmentation and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO™ Toolkit Industry Collaborations\\Representation\n",
    "\n",
    "![](pictures/ov-baidu.png)\n",
    "\n",
    "-----------------\n",
    "\n",
    "![](pictures/ov-medical.png)\n",
    "\n",
    "-------------------\n",
    "\n",
    "![](pictures/ov-medical-broader.png)\n",
    "\n",
    "---------------------\n",
    "\n",
    "![](pictures/ov-summit.png)\n",
    "\n",
    "[Source](https://blogs.intel.com/psg/openvino-toolkit-wins-vision-product-of-the-year-award-in-best-developer-tools-category-at-embedded-vision-summit/)\n",
    "\n",
    "--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Workbench: OpenVINO™ Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Workbench (DL Workbench) is the official OpenVINO™ graphical interface designed to make the production of pretrained deep learning models significantly easier.\r\n",
    "With DL Workbench you can start working with your deep learning model right from your browser: import a model, analyze its performance and accuracy, visualize the outputs, optimize and prepare the model for deployment in a matter of minutes. \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_toolkit-dl-wb-highlighted.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Workbench Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/humans.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_dl_wb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Workbench Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display button for opening DL Workbench\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"<button class=\"wb-button\" style=\"margin-top: 20px;\" onclick=\"window.open(location.origin, '_blank');\">Open DL Workbench</button><style>.wb-button { align-items: center; height: 40px; font-size: 14px; font-weight: 400; line-height: 16px; background-color: #6d0f3f; border: 1px solid #6d0f3f; color: #ffffff; cursor: pointer; border-radius: 4px; padding: 0 20px;}</style>\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import a model\r\n",
    "\r\n",
    "The first step is to import a model. You can either select a model from the Open Model Zoo or import your own model. Open Model Zoo contains a list of publicly available models and Intel pre-trained models that solve many use cases, such as classification, object detection, segmentation and many others. Once you decide which model solves your particular use case, select it and start importing. In case you have trained your model yourself or did not find the required model in the Open Model Zoo, you can upload the files and start the import. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Open Model Zoo Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/import_model_wb_omz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Dataset\r\n",
    "\r\n",
    "Then, we proceed by importing the dataset. Importing the validation dataset means that you need to upload the dataset of supported format suitable for your use case. The dataset can be either annotated or not annotated.\n",
    " 1. Use the following link to download the dataset: [Link](https://github.com/dl-wb-experiments/face-hiding-workshop/files/7043878/dataset.zip).\n",
    "2. Unarchive it;\n",
    "3. Go to DL Workbencs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/validation_dataset_import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Drag & drop the images from the archive to create a not annotated dataset.\r\n",
    "\r\n",
    "![](pictures/custom_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run Inference\r\n",
    "\r\n",
    "Let’s create our first project that will be based on our model with the custom datasets. For that, our final step is to select the environment. Target is a machine that hosts one or several accelerators. Device is a hardware accelerator on which a model is executed. By default, your Local Target is selected. Once we create the project, the first baseline inference happens. During the baseline inference our model is evaluated from the performance perspective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/create_project_selected.png)\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/dashboard-page.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyze the Model\r\n",
    "\r\n",
    "When the inference stage is finished, we can see that our model can process the following number of frames per second with the corresponding latency. Latency is the time required to process one image. The lower the value, the better. Throughput is the number of images (feames) processed per second. Higher throughput value means better performance. Since our model is fully floating-point, we might benefit if we try to execute it in the integer precision. Let's check how our model works and proceed to optimize it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/analyze.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualize Predictions\r\n",
    "\r\n",
    "DL Workbench enables you to visually estimate how well a model recognizes images by testing the model on particular sample images. This considerably enhances the analysis of inference results, giving you an opportunity not only to estimate the performance, but also to visually understand whether the model works correctly and the accuracy is tolerable for client applications.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/predictions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimize the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of the recommended ways to accelerate your model performance is to perform 8-bit integer (INT8) calibration.\r\n",
    "A model in INT8 precision takes up less memory and has higher throughput capacity.\r\n",
    "Often this performance boost is achieved at the cost of a small accuracy reductio\n",
    "\n",
    "8-bit computations (referred to as `int8`) offer better performance compared to the results of inference in higher precision (for example, `fp32`), because they allow loading more data into a single processor instruction. Usually the cost for significant boost is reduced accuracy. However, it is proved that an accuracy drop can be negligible and depends on task requirements, so that the application engineer can set up the maximum accuracy drop that is acceptable.n. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize the Model to Low Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/quantization.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-Trainig Optimization Toolkit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-Training Optimization Toolkit includes standalone command-line tool and Python* API that provide the following key features:\n",
    "\n",
    "* Two supported post-training quantization algorithms: fast [DefaultQuantization](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_default_README.html) and precise [AccuracyAwareQuantization](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_accuracy_aware_README.html).\n",
    "as well as multiple experimental methods including global optimization.\n",
    "* Symmetric and asymmetric quantization schemes. For more details, see the [Quantization](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_README.html) section.\n",
    "* Compression for different hardware targets such as CPU, GPU.\n",
    "* Per-channel quantization for Convolutional and Fully-Connected layers.\n",
    "* Multiple domains: Computer Vision, Recommendation Systems.\n",
    "* Ability to implement custom calibration pipeline via supported [API](https://docs.openvinotoolkit.org/latest/_sample_README.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DefaultQuantization algorithm performs a fast but at the same time accurate INT8 calibration of NNs. It consists of three algorithms that are sequentially applied to a model:\n",
    "*  ActivationChannelAlignment - Used as a preliminary step before quantization and allows you to align ranges of output activations of Convolutional layers in order to reduce the quantization error.\n",
    "*  MinMaxQuantization - This is a vanilla quantization method that automatically inserts `FakeQuantize` operations into the model graph based on the specified  target hardware and initializes them\n",
    "using statistics collected on the calibration dataset.\n",
    "*  BiasCorrection - Adjusts biases of Convolutional and Fully-Connected layers based on the quantization error of the layer in order to make the overall error unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/calibration-int8.png)\n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/int8-page.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the Improvements\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/dashboard-parent-vs-optimized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Profile the Model\r\n",
    "\r\n",
    "The inference of a network is the execution of a computational graph consisting of different operations. Internally, the execution resources are split/pinned into execution streams. \r\n",
    "\r\n",
    "Streams – number of requests running in parallel. Available cores are evenly distributed between the streams. \r\n",
    "\r\n",
    "The neural model can be organized in such a way that the number of input data can vary, which allows to simultaneously process a batch of input data in one pass through the neural network. This can significantly improve the performance.\r\n",
    "\r\n",
    "Batch – number of images propagated to the network at a time.\r\n",
    "During execution of a model, streams, as well as inference requests in a stream, can be distributed inefficiently among cores of hardware, which can reduce model speed. Some target and topology combinations work best with increased batch size and some with multiple parallel inference streams, while some suffer from those options. Predicting performance is extremely complicated; we recommend to try different batch and stream combinations to boost the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/explore-inference.png)\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/inference-table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s recap briefly what you have learned at this stage:\r\n",
    "\r\n",
    "1. What a model is and how it works\r\n",
    "2. How to measure its perdormance\r\n",
    "3. How to accelerate the model using INT8 calibration\r\n",
    "4. How different options affect model performance \r\n",
    "\r\n",
    "Our next step is to apply this knowledge to build the ____ application. Before we proceed, we should determine our model location. For that, go to the Learn OpenVINO tab, select Model Inference with OpenVINO API and copy the model path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Paths to the Model Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/model-paths.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO™ API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to examine a sample application that was created using the [Intel® Distribution of Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit](https://software.intel.com/openvino-toolkit). This tutorial will go step-by-step through the necessary steps to demonstrate object detection on images. Object detection is performed using a pre-trained network and running it using the Intel® Distribution of OpenVINO™ toolkit Inference Engine.\n",
    "\n",
    "Object Detection in Computer Vision is a task of finding objects and locating them in the image.\n",
    "\n",
    "The tutorial guides you through the following steps:\n",
    "\n",
    "1. [Import required modules](#1.-Import-Required-Modules) \n",
    "3. [Configure inference: path to a model and other data](#3.-Configure-an-Inference)\n",
    "4. [Initialize the OpenVINO™ runtime](#4.-Initialize-the-OpenVINO™-Runtime)\n",
    "5. [Read the model](#5.-Read-the-Model)\n",
    "6. [Make the model executable](#6.-Make-the-Model-Executable)\n",
    "7. [Prepare an image for model inference](#7.-Prepare-an-Image-for-Model-Inference)\n",
    "8. [Infer the model](#8.-Infer-the-Model)\n",
    "9. [Show predictions](#9.-Show-Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Modules\n",
    "\n",
    "Import the Python* modules that you will use in the sample code:\n",
    "- [pathlib](https://docs.python.org/3/library/os.html#module-os) is a standard Python module used for filename parsing.\n",
    "- [cv2](https://docs.opencv.org/trunk/) is an OpenCV module used to work with images.\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module used to process images as arrays.\n",
    "- [OpenVINO Inference Engine](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) is an OpenVINO™ Python API module used for inference.\n",
    "- [IPython](https://ipython.readthedocs.io/en/stable/index.html) is an IPython API uused for showing images and videos in the notebook\n",
    "\n",
    "Run the cell below to import the modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore\n",
    "from IPython.display import HTML, Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure an Inference\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: Copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below.\n",
    "#### Required parameters\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "face_detection_model_xml = 'data/models/face-detection-adas-0001.xml'\n",
    "face_detection_model_bin = 'data/models/face-detection-adas-0001.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Parameters\n",
    "\n",
    "Experiment with optional parameters after you go the full workflow of the tutorial.\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**input_image_path**| Path to an input image. Use the `car.bmp` image placed in the directory of the notebook or, if you have imported a dataset in the DL Workbench, copy the path to an image in the dataset.\n",
    "**device**| Specify the [target device](https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Select_Environment.html) to infer on: CPU, GPU, or MYRIAD. Note that the device must be present. For this tutorial, use `CPU` which is known to be present.\n",
    "**prob_threshold**| Probability threshold to filter detection results - PERCENT!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image file. \n",
    "input_image_path = 'data/input_image.JPG'\n",
    "\n",
    "# Device to use\n",
    "device = 'CPU'\n",
    "\n",
    "# Minimum percentage threshold to detect an object\n",
    "prob_threshold = 50\n",
    "\n",
    "print(\n",
    "f'''Configuration parameters settings:\n",
    "    model_xml={face_detection_model_xml},\n",
    "    model_bin={face_detection_model_bin},\n",
    "    input_image_path={input_image_path},\n",
    "    device={device}, \n",
    "    prob_threshold={prob_threshold}''',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize the OpenVINO™ Runtime\n",
    "\n",
    "Once you define the parameters, let's initiate the `IECore` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Inference Engine instance\n",
    "ie_core = IECore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Read the Model\n",
    "\n",
    "Put the IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the model from IR files\n",
    "face_detection_model = ie_core.read_network(model=face_detection_model_xml, weights=face_detection_model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Make the Model Executable\n",
    "\n",
    "Reading a network is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A network loaded to a plugin becomes executable and will be inferred in one of the next steps. \n",
    "\n",
    "After loading, we keep necessary model information such as names of input and output blobs: `input_blob` and `output_blob`. Let's remember the input dimensions of your model:\n",
    "- `n` - input batch size\n",
    "- `c` - number of input channels. Often, it is `1` or `3`, which means that the model expects either a grayscale or a color image.\n",
    "- `h` - input image height\n",
    "- `w` - input image width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store names of input and output of the model\n",
    "face_detector_input_name = next(iter(face_detection_model.input_info))\n",
    "face_detector_output_name = next(iter(face_detection_model.outputs))\n",
    "\n",
    "# Read the input dimensions: n=batch size, c=number of channels, h=height, w=width\n",
    "face_detector_input_shape = face_detection_model.input_info[face_detector_input_name].input_data.shape\n",
    "n, c, face_detector_input_height, face_detector_input_width = face_detector_input_shape\n",
    "\n",
    "print(f'Face Detection model input dimensions: n={n}, c={c}, h={face_detector_input_height}, w={face_detector_input_width}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load the model to the device\n",
    ">NOTE: need documentation https://docs.openvinotoolkit.org/latest/ie_python_api/classie__api_1_1IECore.html#ac9a2e043d14ccfa9c6bbf626cfd69fcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loaded the model into the {device} device.'), \n",
    "face_detector = ie_core.load_network(network=face_detection_model, device_name=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Prepare an Image for Model Inference\n",
    "\n",
    "Now let's read and prepare the input image by resizing and re-arranging its dimensions according to the input dimensions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to load the input image\n",
    "def load_input_image(input_path):   \n",
    "    # Use OpenCV to load the input image\n",
    "    return cv2.imread(input_path)\n",
    "\n",
    "# Define the function to pre-process (resize, transpose, ) the input image\n",
    "def pre_process_input_image(image: np.ndarray, target_height: int, target_width: int) -> np.ndarray:\n",
    "    # Resize the image dimensions from image to model input w x h\n",
    "    resized_image = cv2.resize(image, (target_width, target_height))\n",
    "    \n",
    "    # Change data layout from HWC to CHW\n",
    "    transposed_image = resized_image.transpose((2, 0, 1))\n",
    "    \n",
    "    n = 1 # Batch is always 1 in our case\n",
    "    c = 3 # Channels is always 3 in our case\n",
    "    \n",
    "    # Reshape to input dimensions\n",
    "    reshaped_image = transposed_image.reshape((n, c, target_height, target_width))\n",
    "    return reshaped_image\n",
    "\n",
    "# Define the function to show an image\n",
    "def show_image(image: np.ndarray):\n",
    "    # Encode ndarray\n",
    "    _, data = cv2.imencode('.jpg', image) \n",
    "    \n",
    "    # Create IPython.display.Image instance to display the image in the notebook\n",
    "    image = Image(data=data)\n",
    "\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenCV to load the input image\n",
    "original_image = load_input_image(input_image_path)\n",
    "\n",
    "# Save shape of the original image to scale inference results in the feature\n",
    "original_image_height, original_image_width, *_ = original_image.shape\n",
    "\n",
    "# Prepare the image\n",
    "input_frame = pre_process_input_image(original_image, target_height=face_detector_input_height, target_width=face_detector_input_width)\n",
    "\n",
    "# Display the input image\n",
    "show_image(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Inference\n",
    "NOW INFERENCE! Feeding the prepared image to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_inference_results = face_detector.infer(\n",
    "    inputs={\n",
    "        face_detector_input_name: input_frame\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And show the inference results - it is a dictionary where keyes are names of outputs, the values - results of inference for each output layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(face_detection_inference_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get needed inference results for single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_inference_results = face_detection_inference_results[face_detector_output_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Show Predictions\n",
    "\n",
    "The next step is to parse the inference results and draw boxes over the objects detected in the image.\n",
    "\n",
    "A result of model inference (`face_detection_inference_results`) is an array of predictions. Each prediction `object` has a following structure:\n",
    "\n",
    "- `object[2]`: Confidence level that currently detected object is an instance of the predicted class\n",
    "- `object[3]`: lower x coordinate of the detected object \n",
    "- `object[4]`: lower y coordinate of the detected object\n",
    "- `object[5]`: upper x coordinate of the detected object\n",
    "- `object[6]`: upper y coordinate of the detected object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function of converting the initial results of the face detector into a convenient structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_face_detection_results(inference_results: np.ndarray, original_image_width: int, original_image_height: int) -> list:\n",
    "    # Prepare list to save detected faces \n",
    "    detected_faces = []\n",
    "    \n",
    "    # Iterate through all detected faces\n",
    "    for inference_result in inference_results[0][0]:\n",
    "        \n",
    "        # Get the probability of the detected face and convert it to percent\n",
    "        probability = inference_result[2]\n",
    "        confidence = round(probability * 100, 1)\n",
    "\n",
    "        # If confidence is more than the specified threshold, draw and label the box \n",
    "        if confidence < prob_threshold:\n",
    "            continue\n",
    "\n",
    "        # Get coordinates of the box containing the detected object\n",
    "        xmin = int(inference_result[3] * original_image_width)\n",
    "        ymin = int(inference_result[4] * original_image_height)\n",
    "        xmax = int(inference_result[5] * original_image_width)\n",
    "        ymax = int(inference_result[6] * original_image_height)\n",
    "\n",
    "        detected_face = (xmin, ymin, xmax, ymax, confidence)\n",
    "        detected_faces.append(detected_face)\n",
    "            \n",
    "    return detected_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to process inference results of the face detector - draw boxes in the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process inference results of the face detector \n",
    "def draw_boxes_of_detected_faces(detected_faces: list, original_image: np.ndarray) -> np.ndarray:\n",
    "    processed_image = original_image.copy()\n",
    "    \n",
    "    # Get output results\n",
    "    color = (12.5, 255, 255)\n",
    "    \n",
    "    # Loop through all possible results\n",
    "    for detected_face in detected_faces:\n",
    "        xmin, ymin, xmax, ymax, confidence = detected_face\n",
    "\n",
    "        # Draw the box and label for the detected object\n",
    "        cv2.rectangle(processed_image, (xmin, ymin), (xmax, ymax), color, 4)\n",
    "        cv2.putText(processed_image, \n",
    "                    f'{confidence} %', (xmin, ymin - 7), \n",
    "                    cv2.FONT_HERSHEY_COMPLEX, 1, color, 2)\n",
    "            \n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse detected faces from inference results \n",
    "detected_faces = parse_face_detection_results(face_detection_inference_results, original_image_width, original_image_height)\n",
    "\n",
    "# Draw boxes of the detected facesin the image\n",
    "processed_image = draw_boxes_of_detected_faces(detected_faces, original_image)\n",
    "\n",
    "show_image(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Apply pre-defined blur method to given image at inferred coordinates\n",
    "\n",
    "The idea of this task is to blur all detected faces on the photo. We prepared functions to process detected faces and blur the faces. You need to fill the part by cutting the face from the image, blur it, and past the blurry image to the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all define a function to blur a part of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur(image: np.ndarray) -> np.ndarray:\n",
    "    height, width = image.shape[:2]\n",
    "    pixels_count = 16\n",
    "    \n",
    "    # Resize the image to pixels_count*pixels_count with interpolation to blur the image\n",
    "    resized_image = cv2.resize(image, (pixels_count, pixels_count), interpolation=cv2.INTER_LINEAR)\n",
    "    # Resize the image to original image size\n",
    "    return cv2.resize(resized_image, (width, height), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one more function to process inference results of the face detection network. Use the `blur` function to blur a part of image with a face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_faces(face_detection_inference_result: np.ndarray, original_image: np.ndarray) -> np.ndarray:\n",
    "    # Get original image size  \n",
    "    original_image_height, original_image_width,  _ = original_image.shape\n",
    "    \n",
    "    # Prepare result image to not affect the original image\n",
    "    processed_image = original_image.copy()\n",
    "    \n",
    "    \n",
    "    detected_faces = parse_face_detection_results(face_detection_inference_result, original_image_width, original_image_height)\n",
    "\n",
    "    for detected_face in detected_faces:\n",
    "        xmin, ymin, xmax, ymax, _ = detected_face\n",
    "        # Get the face from the original image \n",
    "        face = original_image[ymin:ymax, xmin:xmax]\n",
    "        # Blur face using blur function \n",
    "        blurry_face = blur(face)\n",
    "        # replace the face to the blurry face in the result image \n",
    "        processed_image[ymin:ymax, xmin:xmax] = blurry_face\n",
    "            \n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now prepare a function to do inference. This function must prepare image for inference (use `pre_process_input_image` for this) and run inference of the image using  `face_detectior`. This function we will use later too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector_inference(image: np.ndarray)-> np.ndarray:\n",
    "    # 1. Prepare the image\n",
    "    input_frame = pre_process_input_image(image, target_width=face_detector_input_width, target_height=face_detector_input_height)\n",
    "\n",
    "    # 2. Infer the model\n",
    "    face_detection_inference_results = face_detector.infer(inputs={face_detector_input_name: input_frame})  \n",
    "    return face_detection_inference_results[face_detector_output_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run face detector inference and blur faces on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_result = face_detector_inference(original_image)\n",
    "\n",
    "# 3. Blur faces on image\n",
    "processed_image = blur_faces(inference_result, original_image)\n",
    "\n",
    "show_image(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Add blurring logic to pre-defined video processor\n",
    "\n",
    "We can blur faces on a image. Noe let's try to blur all faces on a video. For this we will use functions defined early. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths to input and output videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input video file\n",
    "input_video_path = 'data/input.mp4'\n",
    "\n",
    "# Output video file\n",
    "output_video_path = 'data/output.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now open the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream = cv2.VideoCapture(input_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_output_video_stream(input_video_stream: cv2.VideoCapture, output_video_file_path: str) -> cv2.VideoWriter:\n",
    "    width  = int(input_video_stream.get(3))\n",
    "    height = int(input_video_stream.get(4))\n",
    "    video_writer = cv2.VideoWriter(output_video_file_path, cv2.VideoWriter_fourcc(*'avc1'), 20, (width, height))\n",
    "    return video_writer\n",
    "\n",
    "output_video_stream = prepare_output_video_stream(input_video_stream, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while input_video_stream.isOpened():\n",
    "    # 1. Read the next frame from the input video \n",
    "    return_code, original_frame = input_video_stream.read()\n",
    "    if not return_code:\n",
    "        break\n",
    "        \n",
    "    inference_result = face_detector_inference(original_frame)\n",
    "\n",
    "    # 3. Blur faces on image\n",
    "    processed_image = blur_faces(inference_result, original_frame)\n",
    "    \n",
    "    # 3. Write the resulting frame to the output stream\n",
    "    output_video_stream.write(processed_image)\n",
    "    \n",
    "\n",
    "input_video_stream.release()\n",
    "# Save the resulting video\n",
    "output_video_stream.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a source video\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{output_video_path}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: replace each face on the photo with a smile with corresponding emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download a Pretrained Model from the Open Model Zoo\n",
    "\n",
    "OpenVINO™ toolkit includes the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into Intermediate Representation (IR) model files, and the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html), which uses the IR model files to run an inference on hardware devices. The IR model files are created from models trained in popular frameworks, like Caffe\\*, TensorFlow\\*, and others. \n",
    "\n",
    "OpenVINO™ [Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html) downloads common inference models from the [Intel® Open Model Zoo](https://github.com/opencv/open_model_zoo). \n",
    "\n",
    "Let's download the `emotions-recognition-retail-0003` model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ~/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/downloader.py --name emotions-recognition-retail-0003 --precision FP16 --output_dir data/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "emotion_recognition_model_xml = 'data/model/intel/emotions-recognition-retail-0003/FP16/emotions-recognition-retail-0003.xml'\n",
    "emotion_recognition_model_bin = 'data/model/intel/emotions-recognition-retail-0003/FP16/emotions-recognition-retail-0003.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read the model to infernce engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call ie_core.read_network to read the OpenVINO IR model. The method has two arguments:\n",
    "    \n",
    "    1. model - path to the xml model file\n",
    "    2. weights - path to the bin model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_model = ie_core.read_network(model=emotion_recognition_model_xml, weights=emotion_recognition_model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get input name and shape of the model and  output name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get input name and shape of the emotion recognizer. For more information about the model, please refer the link: https://docs.openvinotoolkit.org/latest/omz_models_model_emotions_recognition_retail_0003.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognizer_input_name = next(iter(emotion_recognition_model.input_info))\n",
    "\n",
    "emotion_recognizer_input_shape = emotion_recognition_model.input_info[emotion_recognizer_input_name].input_data.shape\n",
    "n, c, emotion_recognizer_input_height, emotion_recognizer_input_width = emotion_recognizer_input_shape\n",
    "\n",
    "print(f'Input shape of the emotion recognition network is n={n}, c={c}, h={emotion_recognizer_input_height}, w={emotion_recognizer_input_width}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognizer_output_name = next(iter(emotion_recognition_model.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load the network to a device\n",
    "\n",
    "Use the instance of `IECore`.\n",
    "The class `IECore` has a special function called `load_network`, which loads a network to a device.\n",
    "This function prepares the network for the first inference on the device \n",
    "and returns an instance of the network prepared for an inference (execution). \n",
    "This function has many parameters, but in this case, you need to know only about two of them:\n",
    "* `network` - instance of `IENetwork`\n",
    "* `device_name` - string, contains a device name to infer a model on: CPU, GPU and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognizer = ie_core.load_network(emotion_recognition_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Prepare a function to inference an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_recognizer_inference(face_frame: np.ndarray):\n",
    "    prepared_frame = pre_process_input_image(face_frame, target_width=emotion_recognizer_input_width, target_height=emotion_recognizer_input_height)\n",
    "    \n",
    "    # Run the inference how you did it early\n",
    "    inference_results = emotion_recognizer.infer({\n",
    "        emotion_recognizer_input_name: prepared_frame\n",
    "    })\n",
    "    \n",
    "    # For understanding what is the result of inference this model, check documentation \n",
    "    # \n",
    "    return inference_results[emotion_recognizer_output_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the model. Run single inference of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_frame = load_input_image('./data/emotion.jpg')\n",
    "\n",
    "# Infer the model\n",
    "face_detection_inference_results = emotion_recognition_inference(original_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference results is a nested list with prediction for emotions:\n",
    "\n",
    "    - neutral\n",
    "    - happy\n",
    "    - sad\n",
    "    - surprise\n",
    "    - anger\n",
    "    \n",
    " Lets process this results for the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat the nesatead list\n",
    "face_detection_inference_result = face_detection_inference_results.flatten()\n",
    "\n",
    "# Define the emotions\n",
    "emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "\n",
    "show_image(original_frame)\n",
    "\n",
    "print('Inference results:')\n",
    "for index, prediction in enumerate(face_detection_inference_result):\n",
    "    emotion = emotions[index]\n",
    "    print(f'{emotion}:\\t{prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Replace the face with a corresponding smile\n",
    "\n",
    "Now we are ready to replace the detected face with a smile that corresponds to the emotion of this face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "original_image = load_input_image(input_image_path)\n",
    "original_image_height, original_image_width, *_ = original_image.shape\n",
    "\n",
    "# Display the input image\n",
    "print(\"Input image:\")\n",
    "show_image(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the function that returns a coresponding smile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smile_by_index(emotion_inference_result: np.ndarray) -> np.ndarray:\n",
    "    emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "    # Get the index of the emotion with the higher pprobability \n",
    "    emotion_index = np.argmax(emotion_inference_result.flatten()) \n",
    "    smile_path = f'./data/{emotions[emotion_index]}.png'\n",
    "    return cv2.imread(smile_path, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function that puts a smile on top of the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_smile_on_top_of_face(image: np.ndarray, inference_results: np.ndarray, face_xmin:int, face_ymin:int, face_xmax:int, face_ymax:int):\n",
    "\n",
    "    # Get width and height of the face \n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    \n",
    "    # Get smile by inference results\n",
    "    smile = get_smile_by_index(inference_results)\n",
    "    \n",
    "    # Resuze the smilr to face shape\n",
    "    resized_smile = cv2.resize(smile, (width, height))\n",
    "\n",
    "    # Put the smile on top of the face\n",
    "    alpha_s = resized_smile[:, :, 3] / 255.0\n",
    "    alpha_l = 1.0 - alpha_s\n",
    "    for c in range(0, 3):\n",
    "        image[ymin:ymax, xmin:xmax, c] = (alpha_s * resized_smile[:, :, c] + alpha_l * image[ymin:ymax, xmin:xmax, c])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference of face detector to detect faces on the photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_inference_results = face_detector_indeference(original_image)\n",
    "\n",
    "# Parse detected faces\n",
    "faces_coordinates = parse_face_detection_results(face_detection_inference_results, original_image_width, original_image_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put smiles on top of the detected faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the result image\n",
    "processed_image = original_image.copy()\n",
    "\n",
    "for face_coordinates in faces_coordinates:\n",
    "    xmin, ymin, xmax, ymax, confidence = face_coordinates\n",
    "    \n",
    "    # Get the face from the image\n",
    "    face = original_image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "    # Run emotion recognizer \n",
    "    emotion_predictions = emotion_recognizer_inference(face)\n",
    "    \n",
    "    # Replace the face with the smile\n",
    "    put_smile_on_top_of_face(processed_image, emotion_predictions, xmin, ymin, xmax, ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the result image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4:Put smiles on top of the face on the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "original_video_width = int(input_video_stream.get(3))\n",
    "original_video_height = int(input_video_stream.get(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_stream = prepare_output_video_stream(input_video_stream, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while input_video_stream.isOpened():\n",
    "    # 1. Read the next frame from the input video \n",
    "    finish, original_frame = input_video_stream.read()\n",
    "    if not finish:\n",
    "        break\n",
    "        \n",
    "    # 2. apply face replacement from previous step\n",
    "    face_detection_inference_results = face_detector_indeference(original_frame)\n",
    "    \n",
    "    faces_coordinates = parse_face_detection_results(face_detection_inference_results, original_video_width, original_video_height)\n",
    "\n",
    "    for face_coordinates in faces_coordinates:\n",
    "\n",
    "        xmin, ymin, xmax, ymax, _ = face_coordinates\n",
    "        face = original_frame[ymin:ymax, xmin:xmax]\n",
    "\n",
    "        emotion_predictions = emotion_recognizer_inference(face)\n",
    "\n",
    "        put_smile_on_top_of_face(original_frame, emotion_predictions, xmin, ymin, xmax, ymax)\n",
    "    # 3. Write the resulting frame to the output stream\n",
    "    output_video_stream.write(original_frame)\n",
    "    \n",
    "input_video_stream.release()\n",
    "# Save the resulting video\n",
    "output_video_stream.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a source video\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{output_video_path}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
