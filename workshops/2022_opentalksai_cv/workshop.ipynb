{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</figure>\n",
    "<center> <h1>Creating an application for detecting medical masks</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures/preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T09:56:27.382889Z",
     "iopub.status.busy": "2021-08-26T09:56:27.382564Z",
     "iopub.status.idle": "2021-08-26T09:56:27.386235Z",
     "shell.execute_reply": "2021-08-26T09:56:27.385628Z",
     "shell.execute_reply.started": "2021-08-26T09:56:27.382853Z"
    }
   },
   "source": [
    "## 1. [Introduction](#intro)\n",
    "\n",
    "## 2. [OpenVINO™ Overview](#OV-overview)\n",
    "\n",
    "## 3. [OpenVINO™ Deep Learning Workbench](#DL-WB-overview)\n",
    "\n",
    "## 4. [OpenVINO™ API](#OV-API)\n",
    "\n",
    "## 5. [Practice](#tasks)\n",
    "\n",
    "## 6. [Bonus: Next steps with OpenVINO](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction  <a name=\"intro\"></a>\n",
    "\n",
    "### Workshop Contributors\n",
    "\n",
    "<div style=\"display: block; text-align: center;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Demidovskij.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Alexander Demidovskij</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/demid5111\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@demid5111</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Tugaryov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Artyom Tugaryov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/artyomtugaryov\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@artyomtugaryov</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <br>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Savina.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Tatiana Savina</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/tsavina\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@tsavina</a>\n",
    "      </figcaption>\n",
    "    </figure>    \n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Salnikov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Igor Salnikov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/SalnikovIgor\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@SalnikovIgor</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Paniukov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Artur Paniukov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/SalnikovIgor\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@apaniukov</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "### In This Workshop\n",
    "\n",
    "Welcome to the Deep Learning workshop, where you will find out how to start working with pre-trained neural networks and write your own AI application. For that, we will be using the OpenVINO™ framework and its graphical interface Deep Learning Workbench.\n",
    "\n",
    "During this workshop, you will:\n",
    "\n",
    "1. Learn the basics of neural model analysis and optimization:\n",
    "    - what a model is and how it works\n",
    "    - how to measure its performance and analyze the quality\n",
    "    - how to tune the model for enhanced performance\n",
    "2. Write your own AI application that detects faces and classifies whether a person is wearing a mask or not.\n",
    "\n",
    "### The Deep Learning Era\n",
    "\n",
    "Deep Learning recently has gained wide popularity due to significant breakthroughs in the field of artificial neural networks. This has enabled the development of various revolutionary AI applications. With Deep Learning, the algorithm does not need to be taught about the essential features: it can discover features from data on its own using a neural network. Deep Learning algorithms usually use a lot of data and aim to emulate the human brain capacity to observe, learn, and make decisions, particularly for highly complex tasks. \n",
    "\n",
    "![](pictures/why-deep-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"display: flex;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img src=\"pictures/nn-vs-humans-contrast.png\" style=\"height: 300px;max-width: initial;\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Classification of images with high contrast</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://arxiv.org/pdf/1706.06969.pdf\">Source</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img src=\"pictures/nn-vs-humans.png\" style=\"height: 300px;max-width: initial;\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Humans vs. Neural Models the LFW (Labeled Faces in the Wild) dataset</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://arxiv.org/pdf/1404.3840.pdf\">Source</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference\n",
    "\n",
    "To perform a specific AI task, a model is trained on the known data. During training, the model makes predictions about what the data represents. Any error in the prediction is used to strengthen the artificial neuron connections until an acceptable accuracy level is achieved. Training is usually performed only once and requires massive amount of data and powerful computing systems. \n",
    "\n",
    "Inference is the process of using a pre-trained model to make predictions against previously unseen data. Inference is performed on different devices, happens several times, and consists in feeding the images to the model. In this workshop we will work with model inference; in other words, we will execute the model pre-trained for a certain use case.\n",
    "\n",
    "![](pictures/inference_cv.png)\n",
    "\n",
    "\n",
    "Let's take a look at the inference on the real-life example of the vehicle detection model. The model, running alongside the traffic camera, is continuously processing a video to identify the cars approaching the intersection. \n",
    "1. When a car enters at the red light, several photos of this car are taken by the camera and fed to the model. The model finds a license plate on the image and sends it for further processing.\n",
    "2. At the server, the first inference is run to localize the license plate in the image. The second inference is performed to read the characters on the license plate. \n",
    "3. After this, the license plate information is sent to the data center, where an application checks for potential traffic violations. \n",
    "\n",
    "| Device| Data transmission costs | Power consumption|Сomputing resources|\n",
    "|:-----: | :-----: | :-----: | :-----: |\n",
    "|Traffic Camera   | Low| Low | Low |\n",
    "| Gateway Server| Average| Average| Average |\n",
    "|Data Center | High| High| Powerful|\n",
    "\n",
    "![](pictures/system.png)\n",
    "\n",
    "Author: Mark Robins, Intel, [Link](https://www.intel.ru/content/www/ru/ru/artificial-intelligence/posts/deep-learning-training-and-inference.html)\n",
    "\n",
    "Therefore, given the specific task and the cost of data transmission, it is often beneficial to bring the model inference closer to the target where the data is first received. In this case, the pre-trained model requires some accelerating modifications and compression. So let's find out how the OpenVINO™ toolkit can help in achieving better performance of Deep Learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OpenVINO™ Toolkit <a name=\"OV-overview\"></a>\n",
    "\n",
    "1. [How OpenVINO advances AI technologies](#customers) \n",
    "2. [Introduction to the Openvino toolkit](#OV-intro) \n",
    "3. [OpenVINO capabilities](#OV-capabilities)\n",
    "4. [OpenVINO components](#OV-components)\n",
    "5. [OpenVINO Open Model Zoo](#OMZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 How OpenVINO advances AI technologies <a name=\"customers\"></a>\n",
    "\n",
    "The Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit is a comprehensive toolkit for optimizing pre-trained Deep Learning models of various use cases to achieve high performance and prepare them for deployment on Intel® platforms. Based on latest generations of artificial neural networks, including convolutional neural networks (CNNs), recurrent and attention-based networks, the toolkit extends computer vision and non-vision workloads across Intel® hardware, maximizing performance. It accelerates applications with high-performance, AI and deep learning inference deployed from edge to cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino-success-stories.png)\n",
    "![](pictures/sales-collage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Introduction to the OpenVINO™ toolkit <a name=\"OV-intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/ov_motivation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 OpenVINO™ Capabilities <a name=\"OV-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/ov_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 OpenVINO™ Toolkit Components <a name=\"OV-components\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 OpenVINO™ Open Model Zoo <a name=\"OMZ\"></a>\n",
    "\n",
    "A model is a network that has been trained over a set of data using a certain framework. Since Deep learning technologies are used in various industrial\n",
    "applications, it is crucial to have an effective solution for each specific use case. OpenVINO Open Model Zoo provides a range of public and Intel pre-trained models to resolve a variety of different tasks, such as classification, object detection, segmentation and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/omz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Workbench: OpenVINO™ Quickstart <a name=\"DL-WB-overview\"></a>\n",
    "\n",
    "1. [DL Workbench Capabilities](#DL-WB-capabilities)\n",
    "2. [DL Workbench Workflow](#DL-WB-workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Workbench (DL Workbench) is the official OpenVINO™ graphical interface designed to make the production of pre-trained deep learning models significantly easier.\n",
    "With DL Workbench you can start working with your deep learning model right from your browser: import a model, analyze its performance and accuracy, visualize the outputs, optimize and prepare the model for deployment in a matter of minutes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/ov_tools_WB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DL Workbench Capabilities <a name=\"DL-WB-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/DL-WB-flow.png\" width=\"1400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DL Workbench Workflow <a name=\"DL-WB-workflow\"></a>\n",
    "\n",
    "1. [Open DL Workbench](#open-wb)\n",
    "2. [Import the Model](#import-model)\n",
    "3. [Import the Dataset](#import-dataset) \n",
    "4. [Benchmark the Model](#model-inference)\n",
    "5. [Analyze the Model](#analyze-model) \n",
    "6. [Optimize the Model](#optimize-model)\n",
    "7. [Profile the Model](#profile-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display button for opening DL Workbench\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"<button class=\"wb-button\" onclick=\"window.open(location.origin, '_blank');\">Open DL Workbench</button><style>.wb-button { display: flex; width: fit-content; margin: 20px auto; align-items: center; height: 50px; font-size: 18px; font-weight: 400; font-family: inherit; line-height: 20px; background-color: #003C71; border: 1px solid #003C71; color: #ffffff; cursor: pointer; border-radius: 4px; padding: 0 30px; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.15);}</style>\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open DL Workbench <a name=\"open-wb\"></a>\n",
    "\n",
    "To start working with DL Workbench, click **Create Project** button on the start page.\n",
    "\n",
    "![](pictures/start_page_dl_wb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many modern challenges can be potentially addressed with the help of AI. During this workshop we will create an AI-based mask recognition application. Wearing a mask is one of the recommended measures aimed at limiting the community spread of Coronavirus. Our two-phase application will perform face detection, and then classify each face as either with or without mask in images and video.\n",
    " \n",
    "![](pictures/mask_recognition_flow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Model <a name=\"import-model\"></a>\n",
    "\n",
    "The first step on the Create Project page is to import our model. Select and import **face-detection-adas-0001** model from the Open Model Zoo. \n",
    "\n",
    "![](pictures/create_project_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with OpenVINO tools, you need to obtain a model in Intermediate Representation (IR) format. \n",
    "IR is the OpenVINO format of pre-trained model representation with two files:\n",
    "\n",
    "- XML file describing the network topology\n",
    "- BIN file containing weights and biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/face_detection_model_import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the third step, check **I want to Specify Inputs** option. \n",
    "\n",
    "Layout describes the value of each dimension of input tensors. \n",
    "\n",
    "- Type **input_1** in the **Input Name** \n",
    "- Select **NHWC layout** as the **Original Layout**\n",
    "\n",
    "Our model takes **one** (N = 1: number of images in the batch) **color** (C = 3 - number of channels, RGB) image with specified height and width  (H and W = 224). \n",
    "\n",
    "![](pictures/specify_input.png)\n",
    "\n",
    "Click **Validate and Import**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Environment <a name=\"import-dataset-nlp\"></a>\n",
    "\n",
    "On the **Select Environment** page you can choose a hardware accelerator on which the model will be executed. We will analyze our model on a CPU since we have only this device available. Go to the **Next Step**.\n",
    "\n",
    "![](pictures/env_selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset <a name=\"import-dataset\"></a>\n",
    "\n",
    "Validation of the model is always performed against specific data combined into datasets. The data can be in different formats, depending on the task for which the model has been trained. Learn more about dataset formats in the official [documentation](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Dataset_Types.html). \n",
    "\n",
    "In our case, we will take a set of images and use them as the validation dataset:\n",
    "\n",
    "1. Use the following **link to download the dataset**: [Download Dataset](https://github.com/dl-wb-experiments/face-hiding-workshop/files/7081607/dataset.zip).\n",
    "2. Unarchive it.\n",
    "3. Go back to the DL Workbench.\n",
    "4. Click **Import Image Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/create_project_dataset_import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the default images on the **Create Dataset** page:\n",
    "\n",
    "![](pictures/delete_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drag and drop the images from the archive to create the dataset, and click **Import**.\n",
    "\n",
    "![](pictures/dataset_wb_images_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the Model <a name=\"model-inference\"></a>\n",
    "\n",
    "Now that we have imported our model, we want to check how fast it works. For that, let's create our first project. Select the model and the dataset by clicking on them, and click **Create Project**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/create_project_completed.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Model <a name=\"analyze-model\"></a>\n",
    "\n",
    "When the inference stage is finished, we can see the results of running our model on the CPU. We are interested in two metrics: latency and throughput. **Latency** is the time required to process one image. The lower the value, the better. **Throughput** is the number of images (frames) processed per second. Higher throughput value means better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/inference_results_face_detection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch and Stream combinations help accelerate models used in high-performance applications, where many images are being processed simultaneously. **Streams** are the number of instances of your model running simultaneously, and **batches** are the number of input data instances fed to the model. In simple terms, streams are the number of cores, and batches are the number of images processesed by the model. DL Workbench automatically selects the parameters to achieve a near-optimal model performance. You can further accelerate your model by configuring the optimal parameters specific to each accelerator. \n",
    "\n",
    "<center><img src=\"pictures/batch_stream.svg\" width=\"1050\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now let's check how the model works on a test image and try to optimize its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the model predictions, go to **Perform** tab and open **Visualize Output** subtab. Select an image from the previously downloaded dataset, upload this image and click **Visualize**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/visualize_face_detection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the Model <a name=\"optimize-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the common ways to accelerate your model performance is to use 8-bit integer (INT8) calibration.\n",
    "Calibration is the process of lowering the model precision by converting floating-point operations (for example, 32-bit or 16-bit operations) to the nearest 8-bit integer operations. INT8 Calibration accelerates Deep Learning inference while reducing the model size at the cost of accuracy drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/int8_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize the Model to Low Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calibrate a model and then execute it in the INT8 precision, open **Optimize Performance** tab and click **Configure Optimization** button. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/optimize_face_detection.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Default Method** and **Performance Precet** are already selected to achieve better performance results. Click **Optimize**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/optimization_settings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the Improvements\n",
    "\n",
    "The project with the optimized `face-detection-adas-0001` model page opens automatically. To check the performance boost after optimization, go to the **Perform** -> **Optimize Performance** tab. \n",
    "\n",
    "![](pictures/performance_change.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy our face mask detector to embedded devices, we need to make sure that our neural model has high inferencing performance to operate well in a real-time setting. From the optimization results, we see that our model has become X time faster and takes up X times less memory.  Let's proceed to the next step and import our mask detection model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import mask detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the mask detection model using the [link](https://github.com/dl-wb-experiments/workshops/files/7933536/mask_detector.zip).\n",
    "\n",
    "In the DL Workbench, click on the house icon at the top of the screen to **go to the start page**:\n",
    "\n",
    "![](pictures/go_start_page.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a project with the mask detection model:\n",
    "\n",
    "![](pictures/create_project_mask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Open **Import Model** page:\n",
    "\n",
    "![](pictures/mask_import.png)\n",
    "\n",
    "Open the **Original Model** tab, specify **ONNX** framework:\n",
    "\n",
    "![](pictures/import_cv_model.png)\n",
    "\n",
    "Upload the model `.onnx` file and click **Import**:\n",
    "\n",
    "![](pictures/mask_detector_import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the third step - **Convert Model to IR**, select **RGB**  in the **Original Color Space** field:\n",
    "\n",
    "![](pictures/RGB_defined.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check **I want to Specify Inputs** option. \n",
    "\n",
    "1. Layout\n",
    "\n",
    "Layout describes the value of each dimension of input tensors. \n",
    "\n",
    "- Type **input_1** in the **Input Name** \n",
    "- Select **NHWC layout** as the **Original Layout**\n",
    "\n",
    "Our model takes **one** (N = 1: number of images in the batch) **color** (C = 3 - number of channels, RGB) image with specified height and width  (H and W = 224). \n",
    "\n",
    "![](pictures/specify_input.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Means and Scales\n",
    "\n",
    "Check **Use Means** and **Use Scales** options. Enter **127.5** in each field. Then click **Convert and Import**:\n",
    "\n",
    "![](pictures/means_scales.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a project with a mask detector model, we need to import a dataset:\n",
    "\n",
    "1. [Download datset](https://github.com/dl-wb-experiments/workshops/files/8016146/faces_clean.zip).\n",
    "2. Unarchive it.\n",
    "3. Go to the **Select Dataset** page and click **Import Image Dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/import_mask_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Delete the default images.\n",
    "5. Change the Dataset Name to avoid any confusion with the previously imported face detection dataset.\n",
    "6. Drag and drop the images from the archive to create the dataset and click **Import**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/import_masc_detection_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the model and the dataset by clicking on them and **Create your Project**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/create_mask_detector_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new project page with our mask detection model opens:\n",
    "\n",
    "![](pictures/inference_results_face_detection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the **Perform** tab and select **Visualize Output**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/inference_mask_detection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an image from the downloaded dataset and click **Visulize**. To the right, you can see the model prediction, where **#0** means a person with a mask, **#1** - without, and the corresponding model confidence in each prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/vusualize_with_mask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To better understand the prediction making process, in the **Visualization Type** field select **Model Predictions with Importance Map** and click **Visualize** again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/rise.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red image areas indicate the most important pixels for the prediction. In this image, we can see that our model takes into account the parts of the face outside the mask. For our example application, we will continue to use this model, however, in a real-life scenario, this would raise a question of the model trustworthiness.\n",
    "\n",
    "![](pictures/cv_flow_ready.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s briefly recap what you have learned at this stage:\n",
    "\n",
    "1. What a neural model is and how it works\n",
    "2. How to measure its performance\n",
    "3. How to accelerate the model using INT8 calibration\n",
    "4. How to assess model trustworthiness with Explainable AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learn OpenVINO™ API <a name=\"OV-API\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn how to infer a model of object detection use case with OpenVINO™ Python interface and build our application. Object Detection in Computer Vision is the task of finding objects of a certain class and highlighting them with bounding boxes.\n",
    "\n",
    "We will go through the following steps:\n",
    "\n",
    "1. [Import required modules](#Import) \n",
    "2. [Configure inference: path to a model and other data](#Configure)\n",
    "3. [Initialize the OpenVINO™ runtime](#Initialize)\n",
    "4. [Read the model](#Read)\n",
    "5. [Analyze inputs of the model](#Inputs)\n",
    "6. [Configure input data pre processing](#PreProcessing)\n",
    "7. [Compile the Model for the Device](#Compile)\n",
    "8. [Prepare an image for model inference](#Prepare)\n",
    "9. [Infer the model](#Infer)\n",
    "10. [Show predictions](#Show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Import Required Modules <a name=\"Import\"></a>\n",
    "\n",
    "Import the Python* modules that you will use in the sample code:\n",
    "- [OpenVINO](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) is an OpenVINO™ Python API module used for inference\n",
    "- [cv2](https://docs.opencv.org/trunk/) is an OpenCV module used to work with images\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module used to process images as arrays\n",
    "- [IPython](https://ipython.readthedocs.io/en/stable/index.html) is an IPython API used for showing images and videos in the notebook\n",
    "\n",
    "Run the cell below to import the modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openvino.runtime import Core, Layout\n",
    "from openvino.preprocess import PrePostProcessor\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import HTML, Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Configure Inference <a name=\"Configure\"></a>\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: Copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/model_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required parameters\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "face_detection_model_xml = '/Users/atugarev/Developer/resources/models/face_detector/face-detection-adas-0001.xml'\n",
    "face_detection_model_bin = '/Users/atugarev/Developer/resources/models/face_detector/face-detection-adas-0001.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Parameters\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**input_image_path**| Path to an input image. Use the `data/all.jpg` image placed in the directory of the notebook or, if you have imported a dataset in the DL Workbench, copy the path to an image in the dataset.\n",
    "**device**| Specify the [target device](https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Select_Environment.html) to infer on: CPU, GPU, or MYRIAD. Note that the device must be present. For this tutorial, use `CPU` which is known to be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input image file\n",
    "input_image_path = 'data/all.jpg'\n",
    "\n",
    "# Specify the device to use\n",
    "device = 'CPU'\n",
    "\n",
    "print(\n",
    "f'''Configuration parameters settings:\n",
    "    model_xml={face_detection_model_xml},\n",
    "    model_bin={face_detection_model_bin},\n",
    "    input_image_path={input_image_path},\n",
    "    device={device}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Initialize the OpenVINO™ Runtime <a name=\"Initialize\"></a>\n",
    "\n",
    "Once you defined the parameters, let's initiate the `Core` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an OpenVINO instance\n",
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Read the Model <a name=\"Read\"></a>\n",
    "\n",
    "Put the OpenVINO IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the model from OpenVINO IR files\n",
    "face_detection_model = core.read_model(model=face_detection_model_xml, weights=face_detection_model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Analyze inputs of the model <a name=\"Inputs\"></a>\n",
    "\n",
    "After loading the model, we keep necessary model information such as inputs and outputs: `inputs` and `outputs`. Let's recall the input dimensions of your model:\n",
    "- `n` - input batch size\n",
    "- `c` - number of input channels. Often, it is `1` or `3`, which means that the model expects either a grayscale or a color image.\n",
    "- `h` - input image height\n",
    "- `w` - input image width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store input of the model\n",
    "face_detector_input = face_detection_model.input()\n",
    "\n",
    "# Store the name of the model input\n",
    "face_detector_input_name = face_detector_input.any_name\n",
    "\n",
    "# Read the input dimensions: n=batch size, c=number of channels, h=height, w=width\n",
    "face_detector_input_shape = face_detector_input.shape\n",
    "\n",
    "n, c, face_detector_input_height, face_detector_input_width = face_detector_input_shape\n",
    "\n",
    "print(f'Face Detection model input dimensions: n={n}, c={c}, h={face_detector_input_height}, w={face_detector_input_width}')\n",
    "\n",
    "face_detection_output = face_detection_model.output()\n",
    "face_detection_output_shape = face_detection_output.shape\n",
    "\n",
    "print(f'Face Detection model output dimensions {list(face_detection_output_shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/face_detector.png\" width=\"1050\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Configure input data preprocessing <a name=\"PreProcessing\"></a>\n",
    "\n",
    "For proper data processing, it is important to know the layout of the input data expected by the model.\n",
    "Let's check what type of the data layout is expected by the face detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector_input_layout = face_detection_model.inputs[0].node.layout\n",
    "print(f'The face detector expects {face_detector_input_layout} of the input data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, the face detector expects **NCHW** data layout, and we have the **NHWC** input data. Because our face detector expects another type of the layout, we need to prepare the model for our data and specify the layout of input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PreProcessor to define preprocessing manipulations that OpenVINO should do before inference \n",
    "pre_processor = PrePostProcessor(face_detection_model)\n",
    "# We will use OpenCV to work with images\n",
    "# OpenCV reads images in NHWC layout\n",
    "opencv_layout = 'NHWC'\n",
    "# Set input image layout to preprocessor\n",
    "input_image_layout = Layout(opencv_layout)\n",
    "pre_processor.input(0).tensor().set_layout(input_image_layout)\n",
    "# Save changes\n",
    "pre_processor.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Compile the Model for the Device <a name=\"Compile\"></a>\n",
    "\n",
    "Reading a model is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. A model compiled for a device will be inferred in one of the next steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "face_detector = core.compile_model(model=face_detection_model, device_name=device)\n",
    "print(f'Loaded the model into the {device} device.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Prepare an Image for Model Inference <a name=\"Prepare\"></a>\n",
    "\n",
    "Now let's read and prepare the input image by resizing and re-arranging its dimensions according to the input dimensions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function to load the input image\n",
    "def load_input_image(input_path):   \n",
    "    # Use OpenCV to load the input image\n",
    "    return cv2.imread(input_path)\n",
    "\n",
    "# Define the function to pre-process (resize, transpose) the input image\n",
    "def pre_process_input_image(image: np.ndarray, target_height: int, target_width: int) -> np.ndarray:\n",
    "    # Resize the image dimensions from image to model input w x h\n",
    "    resized_image = cv2.resize(image, (target_width, target_height))\n",
    "\n",
    "    n = 1 # Batch is always 1 in our case\n",
    "    c = 3 # Channels is always 3 in our case\n",
    "    \n",
    "    # Reshape to input dimensions\n",
    "    reshaped_image = resized_image.reshape((n, target_height, target_width, c))\n",
    "    return reshaped_image\n",
    "\n",
    "# Define the function to show the image\n",
    "def show_image(image: np.ndarray):\n",
    "    # Encode ndarray\n",
    "    _, data = cv2.imencode('.jpg', image) \n",
    "    \n",
    "    #  Create IPython.display.Image instance to display the image in the notebook\n",
    "    image = Image(data=data)\n",
    "\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_image = load_input_image(input_image_path)\n",
    "show_image(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the shape of the original image to scale inference results in the future\n",
    "original_image_height, original_image_width, *_ = original_image.shape\n",
    "\n",
    "# Prepare the image\n",
    "input_frame = pre_process_input_image(original_image, \n",
    "                                      target_height=face_detector_input_height, \n",
    "                                      target_width=face_detector_input_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Infer the Model <a name=\"Infer\"></a>\n",
    "\n",
    "Now let's proceed to the inference by feeding the prepared image to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_inference_results = face_detector.infer_new_request(\n",
    "    inputs={\n",
    "        face_detector_input_name: input_frame\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we print the inference results: a dictionary, where the keys are the outputs, and the values are the inference results for each output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(face_detection_inference_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the inference results for a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_inference_results = face_detection_inference_results[face_detector.output()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Show Predictions <a name=\"Show\"></a>\n",
    "\n",
    "The next step is to parse the inference results and draw boxes over the objects detected in the image.\n",
    "\n",
    "The result of model inference (`face_detection_inference_results`) is an array of predictions. Each prediction `object` has the following structure:\n",
    "\n",
    "- `object[0]`: information about whether the detected object is a background\n",
    "- `object[1]`: predicted class ID\n",
    "- `object[2]`: сonfidence level that currently detected object is an instance of the predicted class\n",
    "- `object[3]`: lower x coordinate of the detected object \n",
    "- `object[4]`: lower y coordinate of the detected object\n",
    "- `object[5]`: upper x coordinate of the detected object\n",
    "- `object[6]`: upper y coordinate of the detected object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for converting the initial results of the face detector into a more convenient structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_face_detection_results(inference_results: np.ndarray, \n",
    "                                 original_image_height: int,\n",
    "                                 original_image_width: int, \n",
    "                                 prob_threshold: float = 0.8) -> list:\n",
    "    # Prepare a list to save the detected faces \n",
    "    detected_faces = []\n",
    "    \n",
    "    # Iterate through all the detected faces\n",
    "    for inference_result in inference_results[0][0]:\n",
    "        \n",
    "        # Get the probability of the detected face and convert it to percent\n",
    "        probability = inference_result[2]\n",
    "\n",
    "        # If confidence is more than the specified threshold, draw and label the box \n",
    "        if probability < prob_threshold:\n",
    "            continue\n",
    "\n",
    "        # Get coordinates of the box containing the detected object\n",
    "        xmin = int(inference_result[3] * original_image_width)\n",
    "        ymin = int(inference_result[4] * original_image_height)\n",
    "        xmax = int(inference_result[5] * original_image_width)\n",
    "        ymax = int(inference_result[6] * original_image_height)\n",
    "        confidence = round(probability * 100, 1)\n",
    "        \n",
    "        detected_face = (xmin, ymin, xmax, ymax, confidence)\n",
    "        detected_faces.append(detected_face)\n",
    "            \n",
    "    return detected_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the detected faces from the inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse faces  \n",
    "detected_faces = parse_face_detection_results(face_detection_inference_results, original_image_height, original_image_width)\n",
    "detected_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to process inference results of the face detector: draw boxes in the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process inference results of the face detector \n",
    "def draw_boxes_of_detected_faces(detected_faces: list, original_image: np.ndarray) -> np.ndarray:\n",
    "    processed_image = original_image.copy()\n",
    "    \n",
    "    # Get output results\n",
    "    color = (12.5, 255, 255)\n",
    "    \n",
    "    # Loop through all possible results\n",
    "    for detected_face in detected_faces:\n",
    "        xmin, ymin, xmax, ymax, confidence = detected_face\n",
    "\n",
    "        # Draw the box and label for the detected object\n",
    "        cv2.rectangle(processed_image, (xmin, ymin), (xmax, ymax), color, 4)\n",
    "        cv2.putText(processed_image, \n",
    "                    f'{confidence} %', (xmin, ymin - 7), \n",
    "                    cv2.FONT_HERSHEY_COMPLEX, 1, color, 2)\n",
    "            \n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw boxes of the faces detected in the image\n",
    "processed_image = draw_boxes_of_detected_faces(detected_faces, original_image)\n",
    "\n",
    "show_image(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Practice <a name=\"tasks\"></a>\n",
    "\n",
    "1. [Task 1: Crop faces from the image by face detection results](#task1) \n",
    "2. [Task 2: Classify the face using mask detector](#task2)\n",
    "3. [Task 3: Mark people with a mask in the photo](#task3)\n",
    "4. [Task 4: Tack masks on the video](#task4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Crop faces from the image using face detection results <a name=\"task1\"></a>\n",
    "\n",
    "We got the results of the face detector model inferece. To continue building our application, we need to use the face detection results to get the face from the full image. Let's try to do it with the first detected face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the first predicted face from the inferece results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first detected face\n",
    "detected_face_coordinates = detected_faces[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Crop the face from the original image using slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, ymin, xmax, ymax, _ = detected_face_coordinates\n",
    "\n",
    "# To get the face part of the full image, we will use slice \n",
    "face = original_image[ymin:ymax, xmin:xmax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Show the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the face\n",
    "show_image(face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Classify the face with the mask detector <a name=\"task2\"></a>\n",
    "\n",
    "The next step is to run the mask detector inference on the face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the path to the mask detector model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/model_path.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "mask_detection_model_xml = \n",
    "mask_detection_model_bin = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read the model \n",
    "Call core.read_model to read the OpenVINO IR model. The method has two arguments:\n",
    "    \n",
    "    1. model - path to the xml model file\n",
    "    2. weights - path to the bin model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_detection_model = core.read_model(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the input name and shape of the model \n",
    "Get the input name and shape of the mask detector. For more information about the model, refer the to the [documentation](https://github.com/chandrikadeb7/Face-Mask-Detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_detector_input = mask_detection_model.input()\n",
    "mask_detector_input_name = mask_detector_input.any_name\n",
    "\n",
    "mask_detector_input_shape = mask_detector_input.shape\n",
    "mask_detector_input_layout = mask_detector_input.node.layout\n",
    "\n",
    "n, mask_detector_input_height, mask_detector_input_width, c = mask_detector_input_shape\n",
    "\n",
    "mask_detector_output = mask_detection_model.output()\n",
    "mask_detector_output_shape = mask_detector_output.shape\n",
    "\n",
    "print(f'Input layout of the mask detector model is {str(mask_detector_input_layout)}')\n",
    "print(f'Input shape of the mask detector model is n={n}, c={c}, h={mask_detector_input_height}, w={mask_detector_input_width}')\n",
    "\n",
    "print(f'Output shape of the mask detector model is {list(mask_detector_output_shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/mask_detector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compile the model for the device\n",
    "\n",
    "Use the instance of `Core`.\n",
    "The class `Core` has a special function called `compile_model`, which compiles a model for a device.\n",
    "This method prepares the model for inference on the device \n",
    "and returns an instance of the model prepared for an execution. \n",
    "This function has many parameters, but in this case, you need to know only two of them:\n",
    "* `model` - instance of `Model`\n",
    "* `device_name` - string, contains a device name to infer a model on CPU, GPU and other devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_detector = core.compile_model(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prepare the function for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_detector_inference(face_frame: np.ndarray) -> np.ndarray:\n",
    "    # 1. Prepare the image\n",
    "    prepared_frame = pre_process_input_image(face_frame, \n",
    "                                             target_height=mask_detector_input_height, \n",
    "                                             target_width=mask_detector_input_width)\n",
    "    # 2. Infer the model\n",
    "    inference_results = mask_detector.infer_new_request({\n",
    "        mask_detector_input_name: prepared_frame\n",
    "    })\n",
    "    # 3. Return predictions\n",
    "    return inference_results[mask_detector.output()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_detection_inference_results = mask_detector_inference(face)\n",
    "mask_detection_inference_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define the function to process the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_masked(face_frame: np.ndarray) -> bool:\n",
    "    # 1. Infer the model\n",
    "    mask_detection_results = mask_detector_inference(...)\n",
    "    # 2. The first value - probability of the face with a mask, the second - without a mask\n",
    "    return predictions[...] > predictions[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(face)\n",
    "print(f'A person is {\"\" if is_masked(face) else \"not\"} wearing a mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3:  Mark people with and without a mask in the photo <a name=\"task3\"></a>\n",
    "\n",
    "In this task you need to mark people with a mask in the photo. We prepared the function to process the detected faces. You need to fill out the parts of code to cut a face from the image, put a mark on the face, and paste the processed face image to the original photo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Define the function for inference\n",
    "\n",
    "This function will prepare an image for inference (use `pre_process_input_image` to do this) and infer of the image using  `face_detector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def face_detector_inference(image: np.ndarray) -> np.ndarray:\n",
    "    # 1. Prepare the image\n",
    "    input_frame = pre_process_input_image(image, \n",
    "                                          target_width=face_detector_input_width, \n",
    "                                          target_height=face_detector_input_height)\n",
    "\n",
    "    # 2. Infer the model\n",
    "    face_detection_inference_results = face_detector.infer_new_request({\n",
    "        face_detector_input_name: input_frame\n",
    "    }) \n",
    "    # 3. Get the output of the face detector\n",
    "    face_detector_output = face_detector.output()\n",
    "    return face_detection_inference_results[face_detector_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Green check mark for a face with a mask\n",
    "green_mark_path = f'./data/green_mark.png'\n",
    "green_mark = cv2.imread(green_mark_path, -1)\n",
    "\n",
    "# Red cross mark mark for a face without a mask\n",
    "red_cross_path = f'./data/red_cross.png'\n",
    "red_cross = cv2.imread(red_cross_path, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(red_cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define a function that puts a mark over the face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_mark_on_top_of_face(face: np.ndarray, mark: np.ndarray) -> np.ndarray:\n",
    "    result_face = face.copy()\n",
    "    \n",
    "    # Get the width and height of the face \n",
    "    height_face, width_face, _ = face.shape \n",
    "    \n",
    "    min_dim = min(height_face, width_face)\n",
    "    \n",
    "    # Resize the mark to the face shape\n",
    "    resized_mark = cv2.resize(mark, (min_dim, min_dim))\n",
    "\n",
    "    height_mark, width_mark, _ = resized_mark.shape\n",
    "    \n",
    "    yoff = round((height_face - height_mark) / 2)\n",
    "    xoff = round((width_face - width_mark) / 2)\n",
    "    \n",
    "    # Put the mark over the face\n",
    "    alpha_s = resized_mark[:, :, 3] / 255.0\n",
    "    alpha_l = 1.0 - alpha_s\n",
    "    for c in range(0, 3):\n",
    "        result_face[yoff:yoff+height_mark, xoff:xoff+width_mark, c] = \\\n",
    "        alpha_s * resized_mark[:, :, c] + alpha_l * face[yoff:yoff+height_mark, xoff:xoff+width_mark, c]\n",
    "    \n",
    "    return result_face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Infer the model on the given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer the face detector\n",
    "face_detection_inference_result = face_detector_inference(...)\n",
    "\n",
    "# Parse face detection inference results\n",
    "faces_coordinates = parse_face_detection_results(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Process the inference results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the resulting image\n",
    "processed_image = original_image.copy()\n",
    "\n",
    "for face_coordinates in faces_coordinates:\n",
    "    xmin, xmax, ymin, ymax, _ = \n",
    "    \n",
    "    # Get the face from the image\n",
    "    face = \n",
    "\n",
    "    # Run mask detector \n",
    "    is_person_masked = \n",
    "    mark = green_mark if is_person_masked else red_cross\n",
    "        \n",
    "    # Replace the face with the corresponding mark\n",
    "    processed_face = \n",
    "    \n",
    "    processed_image[...] = processed_face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 4: Detect masks on the video <a name=\"task4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define the paths to the input and output videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input video file\n",
    "input_video_path = 'data/input.mp4'\n",
    "\n",
    "# Output video file\n",
    "output_video_path = 'data/output.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Show the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the input video\n",
    "HTML(f\"\"\"<center><video width=\"600\" height=\"400\" controls><source src=\"{input_video_path}\" type=\"video/mp4\"></video></center>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a function to prepare video streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_video_streams(input_video_file_path: str, \n",
    "                          output_video_file_path: str):\n",
    "    input_video_stream = cv2.VideoCapture(input_video_file_path, cv2.CAP_FFMPEG)\n",
    "    width  = int(input_video_stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(input_video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = input_video_stream.get(cv2.CAP_PROP_FPS)\n",
    "    video_writer = cv2.VideoWriter(output_video_file_path, cv2.VideoWriter_fourcc(*'avc1'), fps, (width, height))\n",
    "    return input_video_stream, video_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Process the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream, output_video_stream = prepare_video_streams(input_video_path, output_video_path)\n",
    "\n",
    "while input_video_stream.isOpened():\n",
    "    # 1. Read the next frame from the input video \n",
    "    return_code, original_frame = input_video_stream.read()\n",
    "    if not return_code:\n",
    "        break\n",
    "    \n",
    "    # 2. Get the frame shape\n",
    "    original_frame_height, original_frame_width,  _ = original_frame.shape\n",
    "    \n",
    "    # 2. Infer face detector\n",
    "    inference_results = face_detector_inference(original_frame)\n",
    "    \n",
    "    # 2. Parse coordinates of the detected faces\n",
    "    detected_faces = parse_face_detection_results(inference_results, original_frame_height, original_frame_width)\n",
    "    \n",
    "    for detected_face in detected_faces:\n",
    "        xmin, ymin, xmax, ymax, _ = detected_face\n",
    "        \n",
    "        face = original_frame[ymin:ymax, xmin:xmax]\n",
    "        is_person_masked = is_masked(face)\n",
    "        mark = green_mark if is_person_masked else red_cross\n",
    "        \n",
    "        # Replace the face with the corresponding mark\n",
    "        processed_face = put_mark_on_top_of_face(face, mark)\n",
    "    \n",
    "        original_frame[ymin:ymax, xmin:xmax] = processed_face\n",
    "    \n",
    "    \n",
    "    # 4. Write the resulting frame to the output stream\n",
    "    output_video_stream.write(original_frame)\n",
    "    \n",
    "\n",
    "input_video_stream.release()\n",
    "# Save the resulting video\n",
    "output_video_stream.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Show the resulting video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{output_video_path}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/recap_cv.png\" />\n",
    "\n",
    "#### Workshop repository: [https://github.com/dl-wb-experiments/workshops](https://github.com/dl-wb-experiments/workshops)\n",
    "\n",
    "#### Mask Detector repository: [https://github.com/chandrikadeb7/Face-Mask-Detection](https://github.com/chandrikadeb7/Face-Mask-Detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Next steps with OpenVINO <a name=\"next-steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Try Workbench right now: locally or in DevCloud\n",
    "\n",
    "![](pictures/call-to-action.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. [Intel® Edge AI Certification](https://www.intel.com/content/www/us/en/developer/tools/devcloud/edge/learn/certification.html)\n",
    "\n",
    "![](pictures/edge-ai-certification.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
